 <!-- /\* Font Definitions \*/ @font-face {font-family:Arial; panose-1:2 11 6 4 2 2 2 2 2 4; mso-font-charset:0; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:-536859905 -1073711037 9 0 511 0;} @font-face {font-family:"Cambria Math"; panose-1:2 4 5 3 5 4 6 3 2 4; mso-font-charset:0; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:-536870145 1107305727 0 0 415 0;} @font-face {font-family:"Lucida Grande"; panose-1:2 11 6 0 4 5 2 2 2 4; mso-font-charset:0; mso-generic-font-family:roman; mso-font-format:other; mso-font-pitch:auto; mso-font-signature:0 0 0 0 0 0;} /\* Style Definitions \*/ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-unhide:no; mso-style-qformat:yes; mso-style-parent:""; margin:0in; margin-bottom:.0001pt; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:Arial; mso-fareast-font-family:Arial; mso-ansi-language:EN;} h1 {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:20.0pt; margin-right:0in; margin-bottom:6.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; mso-outline-level:1; font-size:20.0pt; font-family:Arial; mso-font-kerning:0pt; mso-ansi-language:EN; font-weight:normal;} h2 {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:.25in; margin-right:0in; margin-bottom:6.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; mso-outline-level:2; font-size:16.0pt; font-family:Arial; mso-ansi-language:EN; font-weight:normal;} h3 {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:16.0pt; margin-right:0in; margin-bottom:4.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; mso-outline-level:3; font-size:14.0pt; font-family:Arial; color:#434343; mso-ansi-language:EN; font-weight:normal;} h4 {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:14.0pt; margin-right:0in; margin-bottom:4.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; mso-outline-level:4; font-size:12.0pt; font-family:Arial; color:#666666; mso-ansi-language:EN; font-weight:normal;} h5 {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:12.0pt; margin-right:0in; margin-bottom:4.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; mso-outline-level:5; font-size:11.0pt; font-family:Arial; color:#666666; mso-ansi-language:EN; font-weight:normal;} h6 {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:12.0pt; margin-right:0in; margin-bottom:4.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; mso-outline-level:6; font-size:11.0pt; font-family:Arial; color:#666666; mso-ansi-language:EN; font-weight:normal; font-style:italic; mso-bidi-font-style:normal;} p.MsoTitle, li.MsoTitle, div.MsoTitle {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:0in; margin-right:0in; margin-bottom:3.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; font-size:26.0pt; font-family:Arial; mso-fareast-font-family:Arial; mso-ansi-language:EN;} p.MsoSubtitle, li.MsoSubtitle, div.MsoSubtitle {mso-style-unhide:no; mso-style-parent:normal; mso-style-next:normal; margin-top:0in; margin-right:0in; margin-bottom:16.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan lines-together; page-break-after:avoid; font-size:15.0pt; font-family:Arial; mso-fareast-font-family:Arial; color:#666666; mso-ansi-language:EN;} p.MsoAcetate, li.MsoAcetate, div.MsoAcetate {mso-style-noshow:yes; mso-style-priority:99; mso-style-link:"Balloon Text Char"; margin:0in; margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:9.0pt; font-family:"Lucida Grande","serif"; mso-fareast-font-family:Arial; mso-bidi-font-family:Arial; mso-ansi-language:EN;} p.normal, li.normal, div.normal {mso-style-name:normal; mso-style-unhide:no; mso-style-parent:""; margin:0in; margin-bottom:.0001pt; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:Arial; mso-fareast-font-family:Arial; mso-ansi-language:EN;} span.BalloonTextChar {mso-style-name:"Balloon Text Char"; mso-style-noshow:yes; mso-style-priority:99; mso-style-unhide:no; mso-style-locked:yes; mso-style-link:"Balloon Text"; mso-ansi-font-size:9.0pt; mso-bidi-font-size:9.0pt; font-family:"Lucida Grande","serif"; mso-ascii-font-family:"Lucida Grande"; mso-hansi-font-family:"Lucida Grande";} .MsoChpDefault {mso-style-type:export-only; mso-default-props:yes; font-size:11.0pt; mso-ansi-font-size:11.0pt; mso-bidi-font-size:11.0pt; font-family:Arial; mso-ascii-font-family:Arial; mso-fareast-font-family:Arial; mso-hansi-font-family:Arial; mso-bidi-font-family:Arial; mso-ansi-language:EN;} .MsoPapDefault {mso-style-type:export-only; line-height:115%;} @page WordSection1 {size:8.5in 11.0in; margin:.5in .5in .5in .5in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-page-numbers:1; mso-paper-source:0;} div.WordSection1 {page:WordSection1;} /\* List Definitions \*/ @list l0 {mso-list-id:1825008385; mso-list-template-ids:-1314780958;} @list l0:level1 {mso-level-number-format:bullet; mso-level-text:\\25CF; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level2 {mso-level-number-format:bullet; mso-level-text:\\25CB; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level3 {mso-level-number-format:bullet; mso-level-text:\\25A0; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level4 {mso-level-number-format:bullet; mso-level-text:\\25CF; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level5 {mso-level-number-format:bullet; mso-level-text:\\25CB; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level6 {mso-level-number-format:bullet; mso-level-text:\\25A0; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level7 {mso-level-number-format:bullet; mso-level-text:\\25CF; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level8 {mso-level-number-format:bullet; mso-level-text:\\25CB; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} @list l0:level9 {mso-level-number-format:bullet; mso-level-text:\\25A0; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; text-decoration:none; text-underline:none;} ol {margin-bottom:0in;} ul {margin-bottom:0in;} -->  

**Introduction**

In this blog post, we will expand on the Perceptron Algorithm.

The perceptron is a machine learning algorithm.

Machine learning is essentially a method of analyzing data that uses automated analytical model building. A common definition for machine learning is for a system to learn a task without being explicitly programmed to perform said task. As such, machine learning has been the driving engine for artificial intelligence.

Machine learning is often categorized into four types: supervised learning, unsupervised learning, semi-supervised and reinforcement learning.

**Supervised learning**

You know how some practice problems have an answer key? Have you ever tried to study the solution to understand the problem? Welcome to supervised learning where your dataset has the answer keys!

Supervised learning is a machine learning system where the learning algorithm knows the outcome of the training dataset provided. Using these outcomes, the algorithm continually makes predictions until it reaches an acceptable level of accuracy.

Supervised learning involves learning a mapping between between x and y where x is the feature vector (independent variable) and y is the label (dependent variable). If the labeled data, y, is discrete such as boolean values or textual labels, then this becomes a classification problem. If the labeled data, y, is continuous, then this becomes a regression problem.

Supervised learning problems can be divided into classification problems or regression problems. Classification is where the the problem is trying to categorize data into different categories. In supervised learnings the training data outcomes would be the categories that each data point is associated with in the training data set. Regression is where the outcomes are represented by real values like floats.

![](Machine%20Learning%20Blog%20Post_files/image002.gif)

**Unsupervised Learning**

You know how some practice problems do not come with an answer key? The best you can do might be to cluster the problems of your exam into different types of questions, but without the answer key, you don’t know if you’re right or wrong. Welcome to the unsupervised learning which is more difficult than supervised learning. K-Means Clustering is an example of an unsupervised learning algorithm.

Unsupervised learning is when a problem provides only the training data and no corresponding outcomes. The purpose of an unsupervised learning system is to understand the underlying model or distribution of the data rather than understand the data itself and its correlation with outcome.

Unsupervised learning can be divided into clustering and association. In clustering problems, we are trying to figure out the groupings of data. In association problems, we are trying to learn a set of rules that describe the dataset.

**![](Machine%20Learning%20Blog%20Post_files/image004.gif)**

**Semi-Supervised Learning**

Semi-Supervised learning is when the dataset is semi-labeled. Some of the data points might have outcomes while others do not. Semi-supervised learning is a mix of both supervised and unsupervised learning. In these problems we can use unsupervised learning techniques to learn about the structure of the input data and we can also use supervised learning techniques to make predictions on the unlabeled data using the outcomes of the labeled data as training.

**Reinforcement Learning**

Even more challenging than supervised and unsupervised is reinforcement learning which deals with how an agent acts in a dynamic environment, maximizing its reward. For example, OpenAI has used reinforcement learning to compete with expert human players of the video game known as Dota 2. In this example, the ‘agent’ would be the characters and the environment would the entire stage where the characters can roam around. The ‘reward’ that these characters want to maximize would be winning the game.

This blog post shall focus on supervised learning tasks for binary classification.

**Perceptron Algorithm**

Perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is essentially a function that maps its input x to an output value y that is a single binary value. The perceptron algorithm is a linear classifier. This means that if the training set is not linearly separable, the classifier can never be one hundred percent accurate.

The most basic means of classifying data into two categories would be to draw a line that separates them.

![](Machine%20Learning%20Blog%20Post_files/image006.gif)

The diagram above shows two feature vectors (which are the two axises) and two labels (marked as red dots or blue dots). There are three different lines shown that perfectly separates the data points into two categories. In fact, there can be an infinite number of ways to draw a line such that these two labels are separated.

![](Machine%20Learning%20Blog%20Post_files/image008.gif)

The Perceptron algorithm starts by initializing the weights to 0. It then visits each and every data point x to check if the Sign(xTw) is equal to the label of x. Note that Sign refers to the sign function which outputs either 1 or -1. Whenever there’s a mismatch with the predicted label and the actual label, the algorithm adds x to the weights if the actual label is positive or subtracts x to the weights if the actual label is negative.

![](Machine%20Learning%20Blog%20Post_files/image010.gif)

The perceptron algorithm is a building block for neural networks and is considered the simplest feedforward neural network.

**Logistic Regression**

Logistic regression is a statistical model that uses logistic functions to model binary, dependent variables. Unlike the perceptron algorithm which uses the sign function, logistic regression uses the sigmoid function and introduces an important machine learning concept called gradient descent.

We can represent a line as: WTx + b where W is the slope of the line, x is the feature vectors, and b is the y-intercept. More formally, W is often referred to as the weight and b is referred to as the bias.

We already know what x is because that’s our data. But what about the weights and bias?

For binary classification, you may want to know the probability of an event happening given some data.

![](Machine%20Learning%20Blog%20Post_files/image011.gif)

You could try ![](Machine%20Learning%20Blog%20Post_files/image012.gif)which wouldn’t work because a probability must be between 0 and 1.

By applying a sigmoid function ![](Machine%20Learning%20Blog%20Post_files/image013.gif), you can bound the output between 0 and 1.

![Description: _images/sigmoid.png](Machine%20Learning%20Blog%20Post_files/image015.gif)

![](Machine%20Learning%20Blog%20Post_files/image016.gif)

![](Machine%20Learning%20Blog%20Post_files/image017.gif)

**Loss Functions:**

A loss function measures the error between the predicted output with the actual output from the dataset.

There are many types of loss functions:

Hinge Loss - this is what Support Vector Machine would use.

Cross-Entropy Loss (also known as log loss, logistic loss) - this is what a Logistic Regression would use.

![](Machine%20Learning%20Blog%20Post_files/image018.gif)

![](Machine%20Learning%20Blog%20Post_files/image019.gif)

A cost function measures the average error across the entire dataset:

![](Machine%20Learning%20Blog%20Post_files/image020.gif)

Ideally, we would like no errors, a perfect model, a cost function of zero. We want to find w and b that would minimize the cost function as much as possible.

**Gradient Descent** is an algorithm to find w,b that minimizes a convex function:

Repeat {

![](Machine%20Learning%20Blog%20Post_files/image021.gif)

![](Machine%20Learning%20Blog%20Post_files/image022.gif)

}

Both w and b can be initialized to 0. Alternatively, w can also be initialized to a random value.

The alpha value, ![](Machine%20Learning%20Blog%20Post_files/image023.gif), is the learning rate which controls the rate of minimizing towards a local minimum. If alpha is too large, it may miss the local minimum entirely, and if alpha is too small, the training time can be too slow.

![](Machine%20Learning%20Blog%20Post_files/image024.gif)and ![](Machine%20Learning%20Blog%20Post_files/image025.gif)are the gradients of the cost function with respect to the weights and bias.

How do we calculate these gradients?

![](Machine%20Learning%20Blog%20Post_files/image027.gif)

We use a calculus trick known as the Chain Rule as shown above. This involves taking the derivative of the loss function with respect to the activation function (sigmoid in this case), the derivative of the activation function with respect to its inputs z, the derivative of z with respect to the weights or bias, and through Chain Rule, we can derive the derivative of the loss function with respect to the weights and or bias.

It is important to note that gradient descent does not guarantee that it would find the absolute minimum of the convex function, and convex optimization is an active area of research.

**The XOR Problem**

Here’s another labeled dataset:

![](Machine%20Learning%20Blog%20Post_files/image029.gif)

And, down below is the dataset plotted onto a graph. There are only four points, but can you draw a line that separates these them into their respective labels?

**![](Machine%20Learning%20Blog%20Post_files/image031.gif)**

Well, you can’t!

What you see above is the XOR Problem, and it is impossible for the perceptron algorithm to solve this simple problem. There does not exist a single line that can separate these four points into their respective labels.

That is the problem with linear models; they assume that the problem can be linearly separable. Not everything can be so conveniently separated by a straight line.

To solve the XOR problem - and any problems that involves complicated manifolds-, we need to increase the model capacity, meaning that modeling the mapping between the feature vectors and the labels must be more sophisticated than a hyperplane.

By building upon the perceptron algorithm and logistic regression we can create a neural network.

**Neural Network**

A neural network is a machine learning system that is inspired by the brain. It takes after biological neural networks in animal brains. A neural network is essentially stacking multiple perceptrons and can consist of millions of processing nodes that are interconnected. An individual node could be connected to nodes in the layer beneath it from where it receives data and also to nodes in the layers above it that send data. The term Deep Neural Network simply means that there are many layers to the neural network. Deep Learning is a buzzword the media loves to use, but it means exactly the same thing as Deep Neural Network.

![](Machine%20Learning%20Blog%20Post_files/image033.gif)

A neural network has an input layer, at least one hidden layer, and an output layer. Training data is fed to the input layer. The data then goes through all the middle layers and finally comes out at the output layer. The hidden layer(s) are generally in the middle layers. The Perceptron Algorithm can be thought of as a neural network with an input layer, a single hidden layer with one ‘neuron’, and an output layer.

Another way to think about neural networks is that they are essentially a composite function. If x represents the **input layer**, f represents **hidden layer 1**, and g represents a function of the **hidden layer 2**, then the entire neural network could above could be viewed as a composite function: g(f(x)). It’s like Inception except instead of dreams within dreams, we have a function within a function. The more layers a neural networks has, the ‘deeper’ the neural network becomes, and the more sophisticated it can model the mapping between x and y. It is this potential of finding complex models that makes neural networks incredibly powerful, especially with big data.

The activation functions for the hidden layers do not have to use a sigmoid. In fact, tanh (Hyperbolic Tangent) is strictly better than sigmoid because it’s centered towards the origin.

![](Machine%20Learning%20Blog%20Post_files/image034.gif) ![](Machine%20Learning%20Blog%20Post_files/image035.gif)

![](Machine%20Learning%20Blog%20Post_files/image037.gif)

ReLU (Rectified Linear Unit) is usually the default activation function for hidden layers. It is the most widely used activation function.

![](Machine%20Learning%20Blog%20Post_files/image038.gif) ![](Machine%20Learning%20Blog%20Post_files/image039.gif)

![](Machine%20Learning%20Blog%20Post_files/image041.gif)

For binary classification, we would want the output layer to have the sigmoid function to give a binary output.

Logistic Regression uses gradient descent to minimize the cost function. The same holds true for a neural network, but how does one calculate the gradient on a neural network?

Neural networks uses an algorithm known as backpropagation which calculates the gradients using chain rule. In the backpropagation algorithm, we are looking for the minimum value of the error function in weight space. This is done using gradient descent. The output of the algorithm is the weights that minimize the error function. These are considered to be a solution to the problem.

**Tips on Training a Neural Network**

It is common to split the dataset into a training set, validation set (development set), and test set or into just a training set and a test set. The dataset is generally split in an 80:20 ratio where the training set is 80 percent and the test set is 20 percent. If there is a validation set as well, the validation set is 20 percent of the training set. These numbers are not set in stone but a recommendation.

**Normalize the data** – normalizing your data can make it easier and faster to optimize the weights. Here are two very common ways to optimize the data. Both can be use at the same time.

● Subtract Mean: The mean of the dataset becomes zero. This can be done by subtracting every single value in a given feature vector by the mean of the entire vector.

● Normalize Variance: This can be done by dividing each value of a given feature vector by the variance of the vector.

Deep Learning suffers from exploding/diminishing gradients. A partial solution is to that the more features you have, the smaller you want the weights to be. WIth n being the number of features, you can set the variance of the weights to be = 1/n for tanh and 2/n for ReLU.

**Bias and Variance**

Bias and variance is an important concept in the field of machine learning.

If you have High Bias, that indicates underfitting and having a poor training set error. Underfitting means that the model is not sophisticated enough to accurately model the training set. Solutions to this might be having a bigger network, train longer, or have a different neural network architecture

If you have high variance, that indicates overfitting and having a good training set error but a poor validation set error. Solutions to this might be including regularizations which are a collection of techniques to counteract overfitting. **Dropout** is one method of regularization which randomly turns off the some percentage of ‘neurons’ in the hidden layers as the model is training. Another common technique is **Early Stopping**. By plotting to validation set error and the training set error over the course of iterations, this technique picks the point right before the validation set error increases. This technique stops training the right before overfitting happens. Another solution is **Data Augmentation**, meaning increasing the size of the dataset.

Understanding bias and variance is important because it helps you deduce where the problem lies. For instance, if you have high bias, increasing the size of your dataset will not help.

**Mini-Batch Gradient Descent**

Suppose you have a dataset of size m = 5 million. The neural network would have to process all the data before making a step in the gradient descent process. Using mini-batch, you can split up the training/test set into 1,000 each. There would be 5,000 mini-batches. By using mini-batches, the model can converge towards a local minimum faster.

If the mini-batch size is m, that is called Batch Gradient Descent. The problem is that it takes too long for each iteration. This is fine on a small training set (Ex: m <= 2000).

If the mini-batch size is 1, that is called Stochastic Gradient Descent. The problem is that it loses speedup from vectorization.

**Conclusion**

Machine learning is a really broad field and this is just a small part of it. There are a multitude of algorithms and models built for different types of problems. Decision trees for example are a form of supervised learning used to depict every possible outcome of a decision. Another example is the nearest neighbors algorithm. This algorithm approximates how likely it is for a data point to be a part of one grouping or another. Like these, there are many more useful algorithms that are waiting to be studied!

**References**

“Artificial Neural Network.” _Wikipedia_, Wikimedia Foundation, 6 Dec. 2018, en.wikipedia.org/wiki/Artificial\_neural\_network.

“Deep Learning.” _Coursera_, Rice University, www.coursera.org/specializations/deep-learning.

Hardesty, Larry. “Explained: Neural Networks.” _MIT News_, 14 Apr. 2017, news.mit.edu/2017/explained-neural-networks-deep-learning-0414.

“Perceptron.” _Wikipedia_, Wikimedia Foundation, 26 Nov. 2018, en.wikipedia.org/wiki/Perceptron.

Saurabh. “What Is Backpropagation? | Training A Neural Network | Edureka.” _Edureka Blog_, Edureka, 6 Dec. 2018, www.edureka.co/blog/backpropagation/.

“Supervised and Unsupervised Machine Learning Algorithms.” _Machine Learning Mastery_, 22 Sept. 2016, machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/.
